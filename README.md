
## *오목두는 로봇암*
https://github.com/huggingface/lerobot

예전처럼 예상 위치에 그립퍼가 도달하기 위해 각 모터의 움직이는 각도 계산 등의 작업이 필요없고, ros나 slam 등도 몰라도 구현가능하도록 repo가 잘되어있음
다리나 바퀴가 없이 암이 고정되어있고 모터도 6개로 단순한 구성

![[diagram.jpg]](./image/diagram.jpg)
```
vlm + vla로도 가능하나 간단한 구현 위해 각 테스크별 모델로 구성하는 것으로 기획
구동환경이 8G 엣지 디바이스인 관계로 sllm과 vla 등 모든 모델을 메모리에 올리기에는 부족할 것으로 예상되어, chatGPT 등의 api의 사용도 고려
llm은 학습이 불필요
```

#### Process

사용자(optional TTS) :  '오목 두자'
LLM : '그래 무슨 돌을 선택할 거야?'
사용자 : '검은돌'
LLM : '그래 누가 먼저 시작할까?'
사용자 : '니가 먼저 해'

LLM to DQN 오목 조건은 상태로 입력 : agent 돌색상{흰색} 및 사용자 돌색상{검은색}, 먼저 시작하는게 누구인지{사용자 or agent}

Vision Model : 영상에서 이미지를 입력 받아 현재 바둑판의 matrix를 출력, 이전 상태에서 상태가 바뀐 경우(마이턴 여부도 확인) DQN에 matrix(13x13)를 상태로 입력 

DQN to LLM :  matrix 및 상태 조건 입력 받아 이기기 위한 다음 좌표 출력

LLM to VLA : prompt {흰돌을 주워 보드 위 (좌표)에 돌을 둬라} + 영상 입력

VLA : 액션 실행

Vision Model 모델에서 출력한 이전 상태에서 상태가 바뀐 경우
- 오목 미완성 : DQN에 matrix 입력하여 위 과정 반복
- 오목 완성 : LLM에 완성 여부 전달

LLM 승부 결과 출력 : '내가 이김 ㅋㅋ or 분하다 다시 붙자'


## 작업


1. so-arm100 조립 및 jetson 추론환경 구성 > teleoperation 데이터 record > 좌표/돌색상 당 최소 100~200개 이상의 데이터가 필요할 것으로 예상
	- 적정 학습 데이터 수량 파악 : (0,0) 좌표에 원하는 색상의 돌을 놓는 task가 가능한 수량 
	- openvla와 같은 모델을 파인튜닝하거나 isaac sim으로 데이터 증강 시도
2. vla 데이터셋 생성 시간이 오래 걸려 강화학습 모델과 병렬 진행
3. vision 모델 학습 위한 데이터셋 생성 (소량 촬영 후 코드로 생성하는 것으로 고려 중) > 학습
4. 모델입출력 연결하는 agent 생성 > 그냥 파이썬으로 처리하는 것으로 생각 중


## *학습*

- VLA : 영상 + 프롬프트 입력 / 모터 데이터 출력
	- 이미테이션 데이터 생성 필요
	- isaac sim 등 활용하여 합성데이터 생성도 고려
- DQN : 조건 + 바둑판 메트릭스 입력 / 좌표 출력
- Vision : 이미지 입력 / 바둑판 메트릭스 출력
	- cnn 계열 아무거나 vgg16 같은 작은 모델로 할 계획
	- 바둑판 디텍션 후 크롭하여 인풋이미지로 사용하는 것도 고려
	- 연습 삼아 텐서플로우 네트워크 생성해서 학습


## *환경*

- 추론환경
	-  jetson 8g jetpack 6.1
	- 컨테이너 환경 python 3.10
- 학습환경
	- rtx 3080 8g 데스크탑
	- anaconda & venv python 3.10



실사  
![[lerobot.jpg]](./image/lerobot.jpg)