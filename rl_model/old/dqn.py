import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Optional, Tuple, List
import random
from stable_baselines3 import DQN
from stable_baselines3.common.env_util import make_vec_env
# from stable_baselines3.dqn.policies import DQNPolicy
from torch.distributions import Categorical
from stable_baselines3.ppo.policies import MlpPolicy
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.distributions import CategoricalDistribution
import torch.nn.functional as F
import torch


class GomokuSelfPlayEnv(gym.Env):
    def __init__(self, opponent_model=None, board_size=15):
        super().__init__()
        self.board_size = board_size
        self.board = np.zeros((board_size, board_size), dtype=np.int8)
        self.action_space = spaces.Discrete(board_size * board_size)


        self.observation_space = spaces.Dict({
            "obs": spaces.Box(low=0, high=2, shape=(board_size * board_size,), dtype=np.int8),
            "action_mask": spaces.Box(low=0, high=1, shape=(board_size * board_size,), dtype=np.int8)
        })


        self.agent_player = 1  # í•™ìŠµ ëŒ€ìƒ (í‘ëŒ)
        self.opponent_player = 2  # ë™ê²°ëœ ì •ì±… ë˜ëŠ” ì´ì „ ë²„ì „
        self.last_player = None  # ë§ˆì§€ë§‰ ì°©ìˆ˜ì
        self.opponent_model = None  # ë‚˜ì¤‘ì— í• ë‹¹

    def set_opponent_model(self, model):
        self.opponent_model = model

    def _get_action_mask(self):
        return np.array([1 if self._is_valid_action(i) else 0 for i in range(self.board_size ** 2)], dtype=np.int8)

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int8)
        self.done = False
        self.last_player = None

        if random.random() < 0.5:
            center = self.board_size // 2
            self.board[center, center] = self.opponent_player
            self.last_player = self.opponent_player
        else:
            if self.opponent_model:
                obs = self.board.flatten()
                opp_action, _ = self.opponent_model.predict(obs, deterministic=True)
                if self._is_valid_action(opp_action):
                    self._place_stone(opp_action, self.opponent_player)
                    self.last_player = self.opponent_player

        obs = self.board.flatten()
        return {
            "obs": obs,
            "action_mask": self._get_action_mask()
        }, {}

    def valid_actions(self, obs: np.ndarray = None) -> List[int]:
        board = obs.reshape(self.board_size, self.board_size) if obs is not None else self.board
        return [i for i in range(self.board_size * self.board_size)
                if board[i // self.board_size, i % self.board_size] == 0]

    def _is_valid_action(self, action):
        row, col = divmod(action, self.board_size)
        return self.board[row, col] == 0

    def step(self, action):
        if not self._is_valid_action(action):
            return {
                "obs": self.board.flatten(),
                "action_mask": self._get_action_mask()
            }, -1.0, True, False, {"reason": "Invalid move"}

        self._place_stone(action, self.agent_player)
        self.last_player = self.agent_player
        reward, terminated = self._evaluate_board(self.agent_player)
        if terminated:
            return {
                "obs": self.board.flatten(),
                "action_mask": self._get_action_mask()
            }, reward, True, False, {}

        # ìƒëŒ€ í„´
        obs = self.board.flatten()
        valid_actions = self.valid_actions(obs)
        opp_action = None

        if self.opponent_model and valid_actions:
            opp_action, _ = self.opponent_model.predict(obs, deterministic=True)
            if opp_action not in valid_actions:
                opp_action = random.choice(valid_actions)
        elif valid_actions:
            opp_action = random.choice(valid_actions)

        if opp_action is not None:
            self._place_stone(opp_action, self.opponent_player)
            self.last_player = self.opponent_player
            opp_reward, opp_terminated = self._evaluate_board(self.opponent_player)
            if opp_terminated:
                return {
                    "obs": self.board.flatten(),
                    "action_mask": self._get_action_mask()
                }, -opp_reward, True, False, {}

        return {
            "obs": self.board.flatten(),
            "action_mask": self._get_action_mask()
        }, reward, False, False, {}

    def _place_stone(self, action, player):
        row, col = divmod(action, self.board_size)
        self.board[row, col] = player

    def _evaluate_board(self, player):
        chain, check_win = self._max_consecutive(player)
        reward = {2: 0.1, 3: 0.3, 4: 0.7, 5: 1.0}.get(chain, 0.0)
        return reward, chain >= 5

    def _max_consecutive(self, player):
        for y in range(self.board_size):
            for x in range(self.board_size):
                if self.board[y][x] != player:
                    continue
                for dx, dy in [(0, 1), (1, 0), (1, 1), (1, -1)]:
                    count = 0
                    for i in range(5):
                        nx, ny = x + dx * i, y + dy * i
                        if 0 <= nx < self.board_size and 0 <= ny < self.board_size and self.board[ny][nx] == player:
                            count += 1
                        else:
                            break

        if count == 5:
            check_win = True
        else:
            check_win = False
        return count, check_win
    
    def render(self):
        print(self.board)


class MaskedCategorical(Categorical):
    def __init__(self, logits=None, probs=None, mask=None):
        if mask is not None:
            logits = logits.clone()
            logits[~mask] = -1e8  # ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹
        super().__init__(logits=logits, probs=probs)



class MaskedActorCriticPolicy(MlpPolicy):
    def _build(self, lr_schedule):
        # ë¶€ëª¨ í´ë˜ìŠ¤ê°€ latent_pi, latent_vf, value_net, action_net ë“±ì„ ì„¤ì •
        super()._build(lr_schedule)

    def forward(self, obs, deterministic=False):
        # dictë¡œ ì˜ˆìƒí•˜ì§€ë§Œ tensorê°€ ë“¤ì–´ì˜¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if isinstance(obs, dict):
            obs_tensor = obs["obs"]
            action_mask = obs["action_mask"].bool()
        else:
            # ì˜ˆ: obsê°€ tensorë¡œ ë“¤ì–´ì˜¤ë©´ maskë¥¼ ë”°ë¡œ ê³„ì‚°í•˜ê±°ë‚˜, ì˜¤ë¥˜ì²˜ë¦¬
            obs_tensor = obs
            action_mask = torch.ones_like(obs_tensor).bool()  # ì„ì‹œ ë§ˆìŠ¤í¬(ëª¨ë‘ ìœ íš¨)


        # latent vector ìƒì„±
        features = self.extract_features(obs_tensor)
        latent_pi, latent_vf = self.mlp_extractor(features)

        distribution = self._get_action_dist_from_latent(latent_pi)
        masked_dist = MaskedCategorical(logits=distribution.distribution.logits, mask=action_mask)

        if deterministic:
            actions = masked_dist.probs.argmax(dim=1)
        else:
            actions = masked_dist.sample()

        log_prob = masked_dist.log_prob(actions)
        values = self.value_net(latent_vf)

        return actions, values, log_prob












env = DummyVecEnv([lambda: GomokuSelfPlayEnv()])

initial_opponent_model = PPO(
    policy=MaskedActorCriticPolicy,
    env=env,
    learning_rate=1e-4,
    verbose=1,
)
initial_opponent_model.learn(total_timesteps=100_000)










# ì£¼ì¸ê³µ í•™ìŠµ í™˜ê²½ ìƒì„± (opponent ëª¨ë¸ í¬í•¨)
vec_env = make_vec_env(
    GomokuSelfPlayEnv,
    n_envs=320,
    env_kwargs={"opponent_model": initial_opponent_model},
)

# ì£¼ì¸ê³µ ëª¨ë¸ (í•™ìŠµ ëŒ€ìƒ)
main_model = PPO(
    MaskedActorCriticPolicy,
    vec_env,
    learning_rate=1e-4,
    n_steps=1024,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,         # explorationì„ ìœ„í•´ ì¶”ê°€
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    # tensorboard_log="./tensorboard/"
)

# ì´ í•™ìŠµ íƒ€ì„ìŠ¤í… ì„¤ì •
total_timesteps = int(1e+6)
# ëª‡ ìŠ¤í…ë§ˆë‹¤ opponent ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í• ì§€ ì„¤ì •
update_interval = int(1e+5)

# ì£¼ê¸°ì ì¸ í•™ìŠµ ë° opponent ì—…ë°ì´íŠ¸ ë£¨í”„
for step in range(0, total_timesteps, update_interval):
    print(f"â–¶ Learning step {step} ~ {step + update_interval}")

    # í˜„ì¬ ì£¼ì¸ê³µ ëª¨ë¸ì„ í•™ìŠµ
    main_model.learn(total_timesteps=update_interval, reset_num_timesteps=False)

    # opponent ëª¨ë¸ì„ í˜„ì¬ ì£¼ì¸ê³µì˜ ë³µì‚¬ë³¸ìœ¼ë¡œ êµì²´ (frozen copy)
    # opponent ëª¨ë¸ êµì²´
    print("ğŸ” Updating opponent model...")
    main_model.save("temp_main_model.zip")
    updated_opponent_model = DQN.load("temp_main_model.zip")

    # ë²¡í„° í™˜ê²½ ë‚´ opponent ëª¨ë¸ êµì²´
    for env_idx in range(vec_env.num_envs):
        raw_env = vec_env.envs[env_idx].unwrapped
        raw_env.set_opponent_model(updated_opponent_model)


print("âœ… í•™ìŠµ ì™„ë£Œ")
main_model.save("gomoku_dqn_selfplay_final")



train_env = GomokuSelfPlayEnv()
obs, info = train_env.reset()
done = False

while not done:
    action, _states = main_model.predict(obs)
    obs, reward, done, truncated, info = train_env.step(action)
    train_env.render()

    if done:
        winner = train_env.last_player  # ëˆ„ê°€ ì´ê²¼ëŠ”ì§€
        chain, won = train_env._max_consecutive(winner)
        print(f"âœ… ê²Œì„ ì¢…ë£Œ! ìŠ¹ë¦¬ì: {'Agent' if winner == 1 else 'Opponent'} /ëŒìˆ˜ : {chain} / ìŠ¹ë¦¬ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€: {won}")
