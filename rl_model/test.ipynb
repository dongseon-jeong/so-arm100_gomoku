{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "534bb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.ppo.policies import ActorCriticPolicy\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.distributions import CategoricalDistribution\n",
    "from stable_baselines3.common.preprocessing import preprocess_obs\n",
    "from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch as th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40379e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GomokuSelfPlayEnv(gym.Env):\n",
    "    def __init__(self, opponent_model=None, board_size=15):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.board = np.zeros((board_size, board_size), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(board_size * board_size)\n",
    "\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"obs\": spaces.Box(low=0, high=2, shape=(board_size * board_size,), dtype=np.float32),\n",
    "            \"action_mask\": spaces.Box(low=0, high=1, shape=(board_size * board_size,), dtype=np.int8)\n",
    "        })\n",
    "\n",
    "\n",
    "        self.agent_player = 1  # 학습 대상 (흑돌)\n",
    "        self.opponent_player = 2  # 동결된 정책 또는 이전 버전\n",
    "        self.last_player = None  # 마지막 착수자\n",
    "        self.opponent_model = None  # 나중에 할당\n",
    "\n",
    "    def set_opponent_model(self, model):\n",
    "        self.opponent_model = model\n",
    "\n",
    "    def _get_action_mask(self):\n",
    "        return np.array([1 if self._is_valid_action(i) else 0 for i in range(self.board_size ** 2)], dtype=np.int8)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int8)\n",
    "        self.done = False\n",
    "        self.last_player = None\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            center = self.board_size // 2\n",
    "            self.board[center, center] = self.opponent_player\n",
    "            self.last_player = self.opponent_player\n",
    "        else:\n",
    "            if self.opponent_model:\n",
    "                obs = self.board.flatten()\n",
    "                opp_action, _ = self.opponent_model.predict(obs, deterministic=True)\n",
    "                if self._is_valid_action(opp_action):\n",
    "                    self._place_stone(opp_action, self.opponent_player)\n",
    "                    self.last_player = self.opponent_player\n",
    "\n",
    "        obs = self.board.flatten()\n",
    "        return {\n",
    "            \"obs\": self.board.flatten().astype(np.float32),\n",
    "            \"action_mask\": np.array(self.valid_actions_mask(), dtype=np.int8)\n",
    "        }, {}\n",
    "\n",
    "    def valid_actions(self, obs: np.ndarray = None) -> List[int]:\n",
    "        board = obs.reshape(self.board_size, self.board_size) if obs is not None else self.board\n",
    "        return [i for i in range(self.board_size * self.board_size)\n",
    "                if board[i // self.board_size, i % self.board_size] == 0]\n",
    "\n",
    "\n",
    "    def valid_actions_mask(self):\n",
    "        mask = np.zeros(self.board_size * self.board_size, dtype=np.int8)\n",
    "        for idx in self.valid_actions():\n",
    "            mask[idx] = 1\n",
    "        return mask\n",
    "\n",
    "    def _is_valid_action(self, action):\n",
    "        row, col = divmod(action, self.board_size)\n",
    "        return self.board[row, col] == 0\n",
    "\n",
    "    def step(self, action):\n",
    "        if not self._is_valid_action(action):\n",
    "            return {\n",
    "                \"obs\": self.board.flatten().astype(np.float32),\n",
    "                \"action_mask\": np.array(self.valid_actions_mask(), dtype=np.int8)\n",
    "            }, -1.0, True, False, {\"reason\": \"Invalid move\"}\n",
    "\n",
    "        self._place_stone(action, self.agent_player)\n",
    "        self.last_player = self.agent_player\n",
    "        reward, terminated = self._evaluate_board(self.agent_player)\n",
    "        if terminated:\n",
    "            return {\n",
    "                \"obs\": self.board.flatten().astype(np.float32),\n",
    "                \"action_mask\": np.array(self.valid_actions_mask(), dtype=np.int8)\n",
    "            }, reward, True, False, {}\n",
    "\n",
    "        # 상대 턴\n",
    "        obs = self.board.flatten()\n",
    "        valid_actions = self.valid_actions(obs)\n",
    "        opp_action = None\n",
    "\n",
    "        if self.opponent_model and valid_actions:\n",
    "            opp_action, _ = self.opponent_model.predict(obs, deterministic=True)\n",
    "            if opp_action not in valid_actions:\n",
    "                opp_action = random.choice(valid_actions)\n",
    "        elif valid_actions:\n",
    "            opp_action = random.choice(valid_actions)\n",
    "\n",
    "        if opp_action is not None:\n",
    "            self._place_stone(opp_action, self.opponent_player)\n",
    "            self.last_player = self.opponent_player\n",
    "            opp_reward, opp_terminated = self._evaluate_board(self.opponent_player)\n",
    "            if opp_terminated:\n",
    "                return {\n",
    "                    \"obs\": self.board.flatten().astype(np.float32),\n",
    "                    \"action_mask\": np.array(self.valid_actions_mask(), dtype=np.int8)\n",
    "                }, -opp_reward, True, False, {}\n",
    "\n",
    "        return {\n",
    "            \"obs\": self.board.flatten().astype(np.float32),\n",
    "            \"action_mask\": np.array(self.valid_actions_mask(), dtype=np.int8)\n",
    "        }, reward, False, False, {}\n",
    "\n",
    "\n",
    "    def _place_stone(self, action, player):\n",
    "        row, col = divmod(action, self.board_size)\n",
    "        self.board[row, col] = player\n",
    "\n",
    "    def _evaluate_board(self, player):\n",
    "        chain, check_win = self._max_consecutive(player)\n",
    "        reward = {2: 0.1, 3: 0.3, 4: 0.7, 5: 1.0}.get(chain, 0.0)\n",
    "        return reward, chain >= 5\n",
    "\n",
    "    def _max_consecutive(self, player):\n",
    "        for y in range(self.board_size):\n",
    "            for x in range(self.board_size):\n",
    "                if self.board[y][x] != player:\n",
    "                    continue\n",
    "                for dx, dy in [(0, 1), (1, 0), (1, 1), (1, -1)]:\n",
    "                    count = 0\n",
    "                    for i in range(5):\n",
    "                        nx, ny = x + dx * i, y + dy * i\n",
    "                        if 0 <= nx < self.board_size and 0 <= ny < self.board_size and self.board[ny][nx] == player:\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "        if count == 5:\n",
    "            check_win = True\n",
    "        else:\n",
    "            check_win = False\n",
    "        return count, check_win\n",
    "    \n",
    "    def render(self):\n",
    "        print(self.board)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ead18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MaskedCategorical(Categorical):\n",
    "    def __init__(self, logits, mask):\n",
    "        logits = logits.clone()\n",
    "        logits[~mask] = -1e8  # 매우 작은 값으로 설정해서 softmax에서 무시\n",
    "        super().__init__(logits=logits)\n",
    "\n",
    "\n",
    "class MaskedActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # 반드시 먼저 부모 초기화 호출\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.features_extractor = self.make_features_extractor()\n",
    "        self.features_dim = self.features_extractor.features_dim\n",
    "        self.features_extractor_class = base_FlattenExtractor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        \"\"\"\n",
    "        Create the policy and value networks.\n",
    "        Part of the layers can be shared.\n",
    "        \"\"\"\n",
    "        # Note: If net_arch is None and some features extractor is used,\n",
    "        #       net_arch here is an empty list and mlp_extractor does not\n",
    "        #       really contain any layers (acts like an identity module).\n",
    "        self.mlp_extractor = MlpExtractor(\n",
    "            225,\n",
    "            net_arch=self.net_arch,\n",
    "            activation_fn=self.activation_fn,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, obs_dict, deterministic=False):\n",
    "        # obs = obs_dict[\"obs\"].float().to(self.device)\n",
    "        action_mask = obs_dict[\"action_mask\"].bool().to(self.device)\n",
    "\n",
    "\n",
    "        features = self.extract_features(obs_dict)  # obs는 Tensor\n",
    "        \n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "\n",
    "        logits = self.action_net(latent_pi)\n",
    "        dist = MaskedCategorical(logits=logits, mask=action_mask)\n",
    "\n",
    "        actions = torch.argmax(dist.probs, dim=-1) if deterministic else dist.sample()\n",
    "        log_prob = dist.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "\n",
    "        return actions, values, log_prob\n",
    "\n",
    "\n",
    "    def _get_action_dist_from_latent(self, latent_pi):\n",
    "        logits = self.action_net(latent_pi)\n",
    "        return CategoricalDistribution(logits.shape[-1]), logits\n",
    "\n",
    "    def predict(self, observation, state=None, episode_start=None, deterministic=False):\n",
    "\n",
    "        if isinstance(observation, dict):\n",
    "            obs_tensor = {k: torch.FloatTensor(v).to(self.device) for k, v in observation.items()}\n",
    "        else:\n",
    "            raise TypeError(\"Expected observation to be a dict with 'obs' and 'action_mask'\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actions, _, _ = self.forward(obs_tensor, deterministic=deterministic)\n",
    "        return actions.cpu().numpy(), state\n",
    "\n",
    "\n",
    "    def predict_values(self, obs):\n",
    "\n",
    "        features = self.extract_features(obs, self.vf_features_extractor)\n",
    "        latent_vf = self.mlp_extractor.forward_critic(features)\n",
    "        return self.value_net(latent_vf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extract_features(self, obs):\n",
    "        \n",
    "        # obs가 dict인 경우 'obs'만 추출\n",
    "        if isinstance(obs, dict):\n",
    "            obs = obs['obs']\n",
    "\n",
    "        if self.share_features_extractor:\n",
    "            return self.base_extract_features(obs, self.features_extractor)\n",
    "        else:\n",
    "            pi_features = self.base_extract_features(obs, self.pi_features_extractor)\n",
    "            vf_features = self.base_extract_features(obs, self.vf_features_extractor)\n",
    "            return pi_features, vf_features\n",
    "\n",
    "    def base_extract_features(self, obs, features_extractor):\n",
    "        # self.observation_space가 Dict이라면 'obs' 서브공간 사용\n",
    "        if isinstance(self.observation_space, gym.spaces.Dict):\n",
    "            obs_space = self.observation_space.spaces['obs']\n",
    "        else:\n",
    "            obs_space = self.observation_space\n",
    "\n",
    "        preprocessed_obs = self.base_preprocess_obs(obs, obs_space)\n",
    "\n",
    "        return features_extractor(preprocessed_obs)\n",
    "\n",
    "    def base_preprocess_obs(self, obs, observation_space):\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "\n",
    "        # obs가 Box일 경우 그대로 float 처리\n",
    "        if isinstance(observation_space, gym.spaces.Box):\n",
    "            return obs.float()\n",
    "\n",
    "        # obs가 Discrete일 경우 one-hot 인코딩\n",
    "        elif isinstance(observation_space, gym.spaces.Discrete):\n",
    "            return F.one_hot(obs.long(), num_classes=observation_space.n).float()\n",
    "\n",
    "        # obs가 MultiDiscrete일 경우 각 차원을 one-hot 후 concat\n",
    "        elif isinstance(observation_space, gym.spaces.MultiDiscrete):\n",
    "            nvec = observation_space.nvec\n",
    "            return th.cat(\n",
    "                [\n",
    "                    F.one_hot(obs_.long(), num_classes=int(nvec[idx])).float()\n",
    "                    for idx, obs_ in enumerate(th.split(obs.long(), 1, dim=1))\n",
    "                ],\n",
    "                dim=-1,\n",
    "            ).view(obs.shape[0], sum(nvec))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported observation type: {type(observation_space)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "\n",
    "class base_FlattenExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Feature extract that flatten the input.\n",
    "    Used as a placeholder when feature extraction is not needed.\n",
    "\n",
    "    :param observation_space: The observation space of the environment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.Space) -> None:\n",
    "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        observations = observations['obs']\n",
    "        return self.flatten(observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c9a3442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = GomokuSelfPlayEnv()\n",
    "check_env(env)  # 환경 유효성 검사\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\n",
    "    policy=MaskedActorCriticPolicy,\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[dict(pi=[64, 64], vf=[64, 64])],\n",
    "        activation_fn=nn.ReLU,\n",
    "        share_features_extractor=True\n",
    "    )\n",
    ")\n",
    "\n",
    "obs0, info = env.reset()\n",
    "\n",
    "# {k:torch.FloatTensor(v) for k, v in obs0.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "b4516673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 225])\n"
     ]
    }
   ],
   "source": [
    "action0, _states = model.predict(obs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "6b5209f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 225])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[121]\n",
      "[8] [1]\n",
      "0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "action0, _states = model.predict(obs0)\n",
    "obs1, reward, done, truncated, info = env.step(action0)\n",
    "env.render()\n",
    "print(action0)\n",
    "row, col = divmod(action0, 15)\n",
    "print(row, col)\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b89a1a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 225])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[55]\n",
      "[3] [10]\n",
      "0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "action1, _states = model.predict(obs1)\n",
    "obs2, reward, done, truncated, info = env.step(action1)\n",
    "env.render()\n",
    "print(action1)\n",
    "row, col = divmod(action1, 15)\n",
    "print(row, col)\n",
    "print(reward)\n",
    "print(done)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "166f9131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 225])\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[73]\n",
      "[4] [13]\n",
      "0.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "action2, _states = model.predict(obs2)\n",
    "obs3, reward, done, truncated, info = env.step(action2)\n",
    "env.render()\n",
    "print(action2)\n",
    "row, col = divmod(action2, 15)\n",
    "print(row, col)\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fe98a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from typing import Any, Optional, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "from stable_baselines3.common.buffers import DictRolloutBuffer, RolloutBuffer\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import obs_as_tensor, safe_mean\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "\n",
    "SelfOnPolicyAlgorithm = TypeVar(\"SelfOnPolicyAlgorithm\", bound=\"base_OnPolicyAlgorithm\")\n",
    "\n",
    "\n",
    "class base_OnPolicyAlgorithm(BaseAlgorithm):\n",
    "    \"\"\"\n",
    "    The base for On-Policy algorithms (ex: A2C/PPO).\n",
    "\n",
    "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
    "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
    "    :param learning_rate: The learning rate, it can be a function\n",
    "        of the current progress remaining (from 1 to 0)\n",
    "    :param n_steps: The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param gamma: Discount factor\n",
    "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n",
    "        Equivalent to classic advantage when set to 1.\n",
    "    :param ent_coef: Entropy coefficient for the loss calculation\n",
    "    :param vf_coef: Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: The maximum value for the gradient clipping\n",
    "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
    "        instead of action noise exploration (default: False)\n",
    "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
    "        Default: -1 (only sample at the beginning of the rollout)\n",
    "    :param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
    "    :param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation.\n",
    "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
    "        the reported success rate, mean episode length, and mean reward over\n",
    "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
    "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
    "        or not in a Monitor wrapper.\n",
    "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
    "        debug messages\n",
    "    :param seed: Seed for the pseudo random generators\n",
    "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
    "        Setting it to auto, the code will be run on the GPU if possible.\n",
    "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
    "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    rollout_buffer: RolloutBuffer\n",
    "    policy: ActorCriticPolicy\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule],\n",
    "        n_steps: int,\n",
    "        gamma: float,\n",
    "        gae_lambda: float,\n",
    "        ent_coef: float,\n",
    "        vf_coef: float,\n",
    "        max_grad_norm: float,\n",
    "        use_sde: bool,\n",
    "        sde_sample_freq: int,\n",
    "        rollout_buffer_class: Optional[type[RolloutBuffer]] = None,\n",
    "        rollout_buffer_kwargs: Optional[dict[str, Any]] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        monitor_wrapper: bool = True,\n",
    "        policy_kwargs: Optional[dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "        supported_action_spaces: Optional[tuple[type[spaces.Space], ...]] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy=policy,\n",
    "            env=env,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            support_multi_env=True,\n",
    "            monitor_wrapper=monitor_wrapper,\n",
    "            seed=seed,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            supported_action_spaces=supported_action_spaces,\n",
    "        )\n",
    "\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.rollout_buffer_class = rollout_buffer_class\n",
    "        self.rollout_buffer_kwargs = rollout_buffer_kwargs or {}\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        self._setup_lr_schedule()\n",
    "        self.set_random_seed(self.seed)\n",
    "\n",
    "        if self.rollout_buffer_class is None:\n",
    "            if isinstance(self.observation_space, spaces.Dict):\n",
    "                self.rollout_buffer_class = DictRolloutBuffer\n",
    "            else:\n",
    "                self.rollout_buffer_class = RolloutBuffer\n",
    "\n",
    "        self.rollout_buffer = self.rollout_buffer_class(\n",
    "            self.n_steps,\n",
    "            self.observation_space,  # type: ignore[arg-type]\n",
    "            self.action_space,\n",
    "            device=self.device,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda,\n",
    "            n_envs=self.n_envs,\n",
    "            **self.rollout_buffer_kwargs,\n",
    "        )\n",
    "        self.policy = self.policy_class(  # type: ignore[assignment]\n",
    "            self.observation_space, self.action_space, self.lr_schedule, use_sde=self.use_sde, **self.policy_kwargs\n",
    "        )\n",
    "        self.policy = self.policy.to(self.device)\n",
    "        # Warn when not using CPU with MlpPolicy\n",
    "        self._maybe_recommend_cpu()\n",
    "\n",
    "    def _maybe_recommend_cpu(self, mlp_class_name: str = \"ActorCriticPolicy\") -> None:\n",
    "        \"\"\"\n",
    "        Recommend to use CPU only when using A2C/PPO with MlpPolicy.\n",
    "\n",
    "        :param: The name of the class for the default MlpPolicy.\n",
    "        \"\"\"\n",
    "        policy_class_name = self.policy_class.__name__\n",
    "        if self.device != th.device(\"cpu\") and policy_class_name == mlp_class_name:\n",
    "            warnings.warn(\n",
    "                f\"You are trying to run {self.__class__.__name__} on the GPU, \"\n",
    "                \"but it is primarily intended to run on the CPU when not using a CNN policy \"\n",
    "                f\"(you are using {policy_class_name} which should be a MlpPolicy). \"\n",
    "                \"See https://github.com/DLR-RM/stable-baselines3/issues/1245 \"\n",
    "                \"for more info. \"\n",
    "                \"You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.\"\n",
    "                \"Note: The model will train, but the GPU utilization will be poor and \"\n",
    "                \"the training might take longer than on CPU.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "    def collect_rollouts(\n",
    "        self,\n",
    "        env: VecEnv,\n",
    "        callback: BaseCallback,\n",
    "        rollout_buffer: RolloutBuffer,\n",
    "        n_rollout_steps: int,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
    "        The term rollout here refers to the model-free notion and should not\n",
    "        be used with the concept of rollout used in model-based RL or planning.\n",
    "\n",
    "        :param env: The training environment\n",
    "        :param callback: Callback that will be called at each step\n",
    "            (and at the beginning and end of the rollout)\n",
    "        :param rollout_buffer: Buffer to fill with rollouts\n",
    "        :param n_rollout_steps: Number of experiences to collect per environment\n",
    "        :return: True if function returned with at least `n_rollout_steps`\n",
    "            collected, False if callback terminated rollout prematurely.\n",
    "        \"\"\"\n",
    "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
    "        # Switch to eval mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(False)\n",
    "\n",
    "        n_steps = 0\n",
    "        rollout_buffer.reset()\n",
    "        # Sample new weights for the state dependent exploration\n",
    "        if self.use_sde:\n",
    "            self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "        callback.on_rollout_start()\n",
    "\n",
    "        while n_steps < n_rollout_steps:\n",
    "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
    "                # Sample a new noise matrix\n",
    "                self.policy.reset_noise(env.num_envs)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Convert to pytorch tensor or to TensorDict\n",
    "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
    "                actions, values, log_probs = self.policy(obs_tensor)\n",
    "            actions = actions.cpu().numpy()\n",
    "            \n",
    "            # Rescale and perform action\n",
    "            clipped_actions = actions\n",
    "\n",
    "            if isinstance(self.action_space, spaces.Box):\n",
    "                if self.policy.squash_output:\n",
    "                    # Unscale the actions to match env bounds\n",
    "                    # if they were previously squashed (scaled in [-1, 1])\n",
    "                    clipped_actions = self.policy.unscale_action(clipped_actions)\n",
    "                else:\n",
    "                    # Otherwise, clip the actions to avoid out of bound error\n",
    "                    # as we are sampling from an unbounded Gaussian distribution\n",
    "                    clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "            \n",
    "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
    "\n",
    "            self.num_timesteps += env.num_envs\n",
    "\n",
    "            # Give access to local variables\n",
    "            callback.update_locals(locals())\n",
    "            if not callback.on_step():\n",
    "                return False\n",
    "\n",
    "            self._update_info_buffer(infos, dones)\n",
    "            n_steps += 1\n",
    "\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                # Reshape in case of discrete action\n",
    "                actions = actions.reshape(-1, 1)\n",
    "\n",
    "            # Handle timeout by bootstrapping with value function\n",
    "            # see GitHub issue #633\n",
    "            for idx, done in enumerate(dones):\n",
    "                if (\n",
    "                    done\n",
    "                    and infos[idx].get(\"terminal_observation\") is not None\n",
    "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
    "                ):\n",
    "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
    "                    with th.no_grad():\n",
    "                        terminal_value = self.policy.predict_values(terminal_obs)[0]  # type: ignore[arg-type]\n",
    "                    rewards[idx] += self.gamma * terminal_value\n",
    "\n",
    "            rollout_buffer.add(\n",
    "                self._last_obs,  # type: ignore[arg-type]\n",
    "                actions,\n",
    "                rewards,\n",
    "                self._last_episode_starts,  # type: ignore[arg-type]\n",
    "                values,\n",
    "                log_probs,\n",
    "            )\n",
    "            self._last_obs = new_obs  # type: ignore[assignment]\n",
    "            self._last_episode_starts = dones\n",
    "        print(new_obs)\n",
    "        with th.no_grad():\n",
    "            # Compute value for the last timestep\n",
    "            values = self.policy.predict_values(obs_as_tensor(new_obs['obs'], self.device))  # type: ignore[arg-type]\n",
    "\n",
    "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
    "\n",
    "        callback.update_locals(locals())\n",
    "\n",
    "        callback.on_rollout_end()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Consume current rollout data and update policy parameters.\n",
    "        Implemented by individual algorithms.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def dump_logs(self, iteration: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Write log.\n",
    "\n",
    "        :param iteration: Current logging iteration\n",
    "        \"\"\"\n",
    "        assert self.ep_info_buffer is not None\n",
    "        assert self.ep_success_buffer is not None\n",
    "\n",
    "        time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
    "        fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
    "        if iteration > 0:\n",
    "            self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
    "        if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
    "            self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
    "            self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
    "        self.logger.record(\"time/fps\", fps)\n",
    "        self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
    "        self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
    "        if len(self.ep_success_buffer) > 0:\n",
    "            self.logger.record(\"rollout/success_rate\", safe_mean(self.ep_success_buffer))\n",
    "        self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "    def learn(\n",
    "        self: SelfOnPolicyAlgorithm,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"OnPolicyAlgorithm\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfOnPolicyAlgorithm:\n",
    "        iteration = 0\n",
    "\n",
    "        total_timesteps, callback = self._setup_learn(\n",
    "            total_timesteps,\n",
    "            callback,\n",
    "            reset_num_timesteps,\n",
    "            tb_log_name,\n",
    "            progress_bar,\n",
    "        )\n",
    "\n",
    "        callback.on_training_start(locals(), globals())\n",
    "\n",
    "        assert self.env is not None\n",
    "\n",
    "        while self.num_timesteps < total_timesteps:\n",
    "\n",
    "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
    "\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
    "            \n",
    "            # Display training infos\n",
    "            if log_interval is not None and iteration % log_interval == 0:\n",
    "                assert self.ep_info_buffer is not None\n",
    "                self.dump_logs(iteration)\n",
    "            \n",
    "            self.train()\n",
    "\n",
    "        callback.on_training_end()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _get_torch_save_params(self) -> tuple[list[str], list[str]]:\n",
    "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
    "\n",
    "        return state_dicts, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ec29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Any, ClassVar, Optional, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gymnasium import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.buffers import RolloutBuffer\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import FloatSchedule, explained_variance\n",
    "\n",
    "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
    "\n",
    "\n",
    "class base_PPO(base_OnPolicyAlgorithm):\n",
    "    \n",
    "\n",
    "    policy_aliases: ClassVar[dict[str, type[BasePolicy]]] = {\n",
    "        \"MlpPolicy\": ActorCriticPolicy,\n",
    "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
    "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: Union[str, type[ActorCriticPolicy]],\n",
    "        env: Union[GymEnv, str],\n",
    "        learning_rate: Union[float, Schedule] = 3e-4,\n",
    "        n_steps: int = 2048,\n",
    "        batch_size: int = 64,\n",
    "        n_epochs: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        clip_range: Union[float, Schedule] = 0.2,\n",
    "        clip_range_vf: Union[None, float, Schedule] = None,\n",
    "        normalize_advantage: bool = True,\n",
    "        ent_coef: float = 0.0,\n",
    "        vf_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        use_sde: bool = False,\n",
    "        sde_sample_freq: int = -1,\n",
    "        rollout_buffer_class: Optional[type[RolloutBuffer]] = None,\n",
    "        rollout_buffer_kwargs: Optional[dict[str, Any]] = None,\n",
    "        target_kl: Optional[float] = None,\n",
    "        stats_window_size: int = 100,\n",
    "        tensorboard_log: Optional[str] = None,\n",
    "        policy_kwargs: Optional[dict[str, Any]] = None,\n",
    "        verbose: int = 0,\n",
    "        seed: Optional[int] = None,\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "        _init_setup_model: bool = True,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            rollout_buffer_class=rollout_buffer_class,\n",
    "            rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
    "            stats_window_size=stats_window_size,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
    "        # because of the advantage normalization\n",
    "        if normalize_advantage:\n",
    "            assert (\n",
    "                batch_size > 1\n",
    "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
    "\n",
    "        if self.env is not None:\n",
    "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
    "            # when doing advantage normalization\n",
    "            buffer_size = self.env.num_envs * self.n_steps\n",
    "            assert buffer_size > 1 or (\n",
    "                not normalize_advantage\n",
    "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
    "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
    "            untruncated_batches = buffer_size // batch_size\n",
    "            if buffer_size % batch_size > 0:\n",
    "                warnings.warn(\n",
    "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
    "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
    "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
    "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
    "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
    "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
    "                )\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.clip_range = clip_range\n",
    "        self.clip_range_vf = clip_range_vf\n",
    "        self.normalize_advantage = normalize_advantage\n",
    "        self.target_kl = target_kl\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self._setup_model()\n",
    "\n",
    "    def _setup_model(self) -> None:\n",
    "        super()._setup_model()\n",
    "\n",
    "        # Initialize schedules for policy/value clipping\n",
    "        self.clip_range = FloatSchedule(self.clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            if isinstance(self.clip_range_vf, (float, int)):\n",
    "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
    "\n",
    "            self.clip_range_vf = FloatSchedule(self.clip_range_vf)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Update policy using the currently gathered rollout buffer.\n",
    "        \"\"\"\n",
    "        # Switch to train mode (this affects batch norm / dropout)\n",
    "        self.policy.set_training_mode(True)\n",
    "        # Update optimizer learning rate\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "        # Compute current clip range\n",
    "        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]\n",
    "        # Optional: clip range for the value function\n",
    "        if self.clip_range_vf is not None:\n",
    "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]\n",
    "\n",
    "        entropy_losses = []\n",
    "        pg_losses, value_losses = [], []\n",
    "        clip_fractions = []\n",
    "\n",
    "        continue_training = True\n",
    "        # train for n_epochs epochs\n",
    "        for epoch in range(self.n_epochs):\n",
    "            approx_kl_divs = []\n",
    "            # Do a complete pass on the rollout buffer\n",
    "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
    "                actions = rollout_data.actions\n",
    "\n",
    "                if isinstance(self.action_space, spaces.Discrete):\n",
    "                    # Convert discrete action from float to long\n",
    "                    actions = rollout_data.actions.long().flatten()\n",
    "\n",
    "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "                values = values.flatten()\n",
    "                # Normalize advantage\n",
    "                advantages = rollout_data.advantages\n",
    "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
    "                if self.normalize_advantage and len(advantages) > 1:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "                # ratio between old and new policy, should be one at the first iteration\n",
    "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
    "\n",
    "                # clipped surrogate loss\n",
    "                policy_loss_1 = advantages * ratio\n",
    "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
    "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
    "\n",
    "                # Logging\n",
    "                pg_losses.append(policy_loss.item())\n",
    "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
    "                clip_fractions.append(clip_fraction)\n",
    "\n",
    "                if self.clip_range_vf is None:\n",
    "                    # No clipping\n",
    "                    values_pred = values\n",
    "                else:\n",
    "                    # Clip the difference between old and new value\n",
    "                    # NOTE: this depends on the reward scaling\n",
    "                    values_pred = rollout_data.old_values + th.clamp(\n",
    "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
    "                    )\n",
    "                # Value loss using the TD(gae_lambda) target\n",
    "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
    "                value_losses.append(value_loss.item())\n",
    "\n",
    "                # Entropy loss favor exploration\n",
    "                if entropy is None:\n",
    "                    # Approximate entropy when no analytical form\n",
    "                    entropy_loss = -th.mean(-log_prob)\n",
    "                else:\n",
    "                    entropy_loss = -th.mean(entropy)\n",
    "\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "\n",
    "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "\n",
    "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
    "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
    "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
    "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
    "                with th.no_grad():\n",
    "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
    "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
    "                    approx_kl_divs.append(approx_kl_div)\n",
    "\n",
    "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
    "                    continue_training = False\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
    "                    break\n",
    "\n",
    "                # Optimization step\n",
    "                self.policy.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Clip grad norm\n",
    "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "                self.policy.optimizer.step()\n",
    "\n",
    "            self._n_updates += 1\n",
    "            if not continue_training:\n",
    "                break\n",
    "\n",
    "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
    "\n",
    "        # Logs\n",
    "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
    "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
    "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
    "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
    "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
    "        self.logger.record(\"train/loss\", loss.item())\n",
    "        self.logger.record(\"train/explained_variance\", explained_var)\n",
    "        if hasattr(self.policy, \"log_std\"):\n",
    "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/clip_range\", clip_range)\n",
    "        if self.clip_range_vf is not None:\n",
    "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
    "\n",
    "    def learn(\n",
    "        self: SelfPPO,\n",
    "        total_timesteps: int,\n",
    "        callback: MaybeCallback = None,\n",
    "        log_interval: int = 1,\n",
    "        tb_log_name: str = \"PPO\",\n",
    "        reset_num_timesteps: bool = True,\n",
    "        progress_bar: bool = False,\n",
    "    ) -> SelfPPO:\n",
    "        return super().learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            log_interval=log_interval,\n",
    "            tb_log_name=tb_log_name,\n",
    "            reset_num_timesteps=reset_num_timesteps,\n",
    "            progress_bar=progress_bar,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b1f50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dqn\\lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GomokuSelfPlayEnv()\n",
    "\n",
    "model = base_PPO(\n",
    "    policy=MaskedActorCriticPolicy,\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[dict(pi=[64, 64], vf=[64, 64])],\n",
    "        activation_fn=nn.ReLU,\n",
    "        share_features_extractor=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f36400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('action_mask', array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]], dtype=int8)), ('obs', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.]], dtype=float32))])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected dict, got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[81], line 269\u001b[0m, in \u001b[0;36mbase_PPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    262\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 329\u001b[0m, in \u001b[0;36mbase_OnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 329\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[80], line 264\u001b[0m, in \u001b[0;36mbase_OnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_obs)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# Compute value for the last timestep\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    266\u001b[0m rollout_buffer\u001b[38;5;241m.\u001b[39mcompute_returns_and_advantage(last_values\u001b[38;5;241m=\u001b[39mvalues, dones\u001b[38;5;241m=\u001b[39mdones)\n\u001b[0;32m    268\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dqn\\lib\\site-packages\\stable_baselines3\\common\\policies.py:761\u001b[0m, in \u001b[0;36mActorCriticPolicy.predict_values\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    755\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m    Get the estimated values according to the current policy given the observations.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m    :param obs: Observation\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m    :return: the estimated values.\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 761\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvf_features_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    762\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(features)\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dqn\\lib\\site-packages\\stable_baselines3\\common\\policies.py:130\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    :return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     preprocessed_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dqn\\lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:110\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[1;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mPreprocess observation to be to a neural network.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03mFor images, it normalizes the values by dividing them by 255 (to have values in [0, 1])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mDict):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# Do not modify by reference the original observation\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     preprocessed_obs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, _obs \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected dict, got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d877fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
